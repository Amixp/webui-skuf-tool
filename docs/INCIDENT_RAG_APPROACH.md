# RAG-агент для автоматизации инцидент-менеджмента

**Источник:** [OTUS Journal - RAG-агент для автоматизации инцидент-менеджмента](https://otus.ru/journal/rag-agent-dlya-avtomatizacii-incident-menedzhmenta/)

**Автор:** Александр Летуновский, руководитель центра, Сбербанк страхование

## Проблематика

Крупные организации сталкиваются с большим числом ИТ-инцидентов: иногда – с тысячами в месяц. Инциденты нередко повторяются, однако найти похожий случай в базе знаний или в системе регистрации непросто: стандартный поиск по ключевым словам часто неэффективен, а помнить детали всех инцидентов невозможно.

При поступлении нового инцидента инженеру трудно выяснить, был ли подобный инцидент раньше, и как его разрешили. Многие знания рассеиваются по прошлым тикетам и не используются вновь.

## Цель работы

Автоматизировать сопоставление новых инцидентов с решёнными ранее и предоставлять инженерам рекомендации по решению в виде комментария в тикете.

## Ограничения и особенности внедрения

### I. Среда полностью офлайн
- Нет доступа к облачным сервисам (OpenAI API, управляемые векторные базы)
- Все компоненты должны быть размещены локально
- Обновления моделей требуют отдельной процедуры загрузки

### II. Отсутствие GPU на этапе MVP
- Сервер оснащён только CPU
- Выбор легковесных и оптимизированных моделей

## Архитектура решения

Процесс состоит из семи шагов:

1. **Получение инцидента:** агент отслеживает появление нового инцидента в системе Service Desk и через API выгружает его описание в текстовом формате: HTML-разметка очищается до простого текста.

2. **Дополнение описания:** чтобы уточнить проблему, исходное описание инцидента при необходимости автоматически дополняется или переформулируется с помощью большой языковой модели. Например, извлечь ключевые симптомы, дополнить короткое описание, удалить лишние детали из описания.

3. **Векторизация описания:** текст инцидента (как в оригинале, так и переформулированный вариант) преобразуется в векторное представление – **embedding** – при помощи модели эмбеддинга.

4. **Поиск похожих инцидентов:** полученный вектор сравнивается с векторами ранее решённых инцидентов, хранящимися в векторной базе данных, с использованием метрики косинусного расстояния. В результате извлекаются несколько наиболее похожих описаний из базы (топ-5 по оригинальному тексту и топ-5 по переформулированному).

5. **Генерация рекомендаций:** собранная информация подаётся на вход большой языковой модели – формируется запрос (prompt), содержащий описание новой проблемы и сведения о найденных похожих инцидентах (описания и решения). LLM анализирует этот контекст и генерирует **рекомендацию** – комментарий с предположением причины проблемы и советом по её решению.

6. **Обновление базы знаний:** векторное представление нового инцидента сохраняется в векторной базе (вместе с текстом описания и решения после закрытия инцидента) для использования в будущем.

7. **Публикация решения:** сгенерированная LLM рекомендация и список ссылок на похожие инциденты автоматически отправляются в систему Service Desk в виде комментария к соответствующему тикету, где его видят инженеры.

## Ключевые компоненты

### Извлечение текста инцидентов из HTML

Интеграция реализована через REST API: агент периодически опрашивает систему на наличие новых тикетов или получает уведомление о них. Для каждого нового инцидента с помощью API вытягивается текст поля «Описание» (Description). Поскольку в Service Desk описания хранятся с разметкой HTML, этот контент приводится к простому тексту: удаляются HTML-теги, лишние символы, приводятся в порядок списки и форматирование. Разбор HTML выполняется с помощью библиотеки парсинга BeautifulSoup4.

**Пример кода:**
```python
def clean_html(html_content):
    """Remove HTML tags from the content."""
    if html_content is None:
        return None
    soup = BeautifulSoup(html_content, 'html.parser')
    return soup.get_text(strip=True)
```

### Векторизация описаний инцидентов

Векторное представление – это набор чисел (координат векторов в многомерном пространстве), которые отражают смысл текста. Для вычисления степени близости векторов используется метрика косинусного расстояния (cosine similarity). Эта метрика возвращает значение от 0 до 1, отражающее степень сходства.

**Выбор модели эмбеддинга**

Протестированные модели:
- **all-MiniLM-L6-v2**: компактная модель размерности 384, давала хорошую скорость, но уступала по качеству
- **LaBSE-en-ru**: модель от Google с размерностью вектора 768, содержащая только токены русского и английского языков. Модель очень хорошо справлялась с текстами инцидентов
- **ru-en-RoSBERTa**: русско-английская модель с размерностью векторов 1024, специально дообученная для работы с двуязычными текстами

**Победитель: ru-en-RoSBERTa** - показала наилучшее ранжирование и точность сопоставления, особенно при анализе инцидентов, содержащих технические термины, коды ошибок и смешанную лексику.

**Пример кода:**
```python
# Загрузка модели
model = SentenceTransformer('ru-en-RoSBERTa')

# Векторизация текста
vectors_description = model.encode(df['description'].tolist())
```

**Переформулировка описаний с LLM**

Особенностью подхода стало использование большой языковой модели (LLM) для улучшения качества векторизации. Иногда описания инцидентов бывают скудными или неочевидными, или наоборот слишком длинными. В таких случаях даже хорошая модель эмбеддинга может не понять суть и «выдать» не самых релевантных соседей.

Поэтому был реализован шаг **дополнительной обработки описания с помощью LLM**: перед векторизацией агент может отправить языковой модели запрос, чтобы та переформулировала, дополнила или сжала описание, подчеркнув ключевые детали. Получив краткий текст ошибки, модель генерирует более развернутое описание проблемы, которое затем тоже превращается в вектор. Далее поиск похожих инцидентов производится **и по исходному, и по переформулированному описанию** с выбором наиболее близких по смыслу инцидентов.

**Пример промта для LLM:**
```
Вы являетесь ИТ-специалистом, который обрабатывает ИТ-инциденты. 

Ваша задача — обработать описания ИТ-инцидентов, чтобы улучшить качество извлечения похожих текстов из векторного хранилища. Следуйте этим инструкциям:

Если запрос совсем короткий и недостаточно информативен, дополните его, добавив возможные причины, контекст или дополнительные детали.

Если запрос длинный и содержит много лишних деталей, в том числе повторяющихся — сократите его, оставив только ключевую информацию.

Уберите излишние подробности, но сохраните контекст, необходимый для понимания проблемы.

Верните только измененное описание инцидента, ничего больше.

Описание инцидента:

{query}

Ваш вариант:
```

### Векторная база данных

Для хранения всех эмбеддингов прошлых инцидентов и выполнения поиска по ним используется **PGVector** - расширение для PostgreSQL, которое добавляет тип данных «Vector», обеспечивая хранение эмбеддингов в виде векторов и поиск ближайших векторов по косинусному расстоянию.

**Преимущества PGVector:**
- Привычная работа с СУБД PostgreSQL и SQL запросами
- Надёжность
- Масштабируемость

**Пример SQL-кода для создания таблицы:**
```sql
CREATE TABLE IF NOT EXISTS incidents (
    id SERIAL PRIMARY KEY,
    number INTEGER,
    creation_date DATE,
    report_text TEXT,
    theme TEXT,
    description TEXT,
    service TEXT,
    embedding_description vector(1024),
    llm_response TEXT
);
```

**Пример SQL-кода для поиска похожих инцидентов:**
```python
def search_similar_vectors(query, k=5, distance_threshold=0.5):
    query_vector = model.encode([query])[0].tolist()
    try:
        cur.execute("""
            SELECT id, number, creation_date, report_text, theme, description, service, embedding_combined,
                (embedding_combined <=> %s::vector) AS distance
            FROM incidents
            WHERE embedding_combined <=> %s::vector < %s
            ORDER BY distance
            LIMIT %s
        """, (query_vector, query_vector, distance_threshold, k))
        results = cur.fetchall()
        return results
    except Exception as e:
        conn.rollback()
        print(f"Ошибка при поиске векторов: {e}")
        return []
```

**Настройка порога:** Изначально допускались совпадения с близостью >0.5, но это приводило к попаданию нерелевантных случаев. После наблюдения за работой сервиса на протяжение пары недель порог был снижен до **0.15**. Качество отбора существенно улучшилось – в список кандидатов стали попадать действительно только похожие тикеты.

### Выбор и использование LLM: Ollama + Gemma 3

Важнейший компонент системы – **большая языковая модель** (LLM). Модель используется дважды: для улучшения описания и финальной генерации текста рекомендации.

**Ограничения:**
- Офлайн-среда (без доступа к облачным API)
- Отсутствие GPU на этапе MVP
- Ограничения по числу параметров для приемлемой скорости

**Выбранная модель: Gemma 3-4b-instruct**
- LLM с 4 миллиардами параметров
- Обучена следовать инструкциям (instruct-tuned)
- Загружена в **квантованном виде (4-бит)**, что снизило требования к памяти до нескольких гигабайт
- Использование 4-битной квантованной модели позволило запускать инференс LLM на CPU достаточно быстро – создание рекомендации занимало примерно минуту

**Инструмент: Ollama**
- Оптимизированный для LLM локальный сервис
- Предоставляет простой API для генерации текста
- Облегчает использование моделей: загрузка модели в память, управление контекстом, ускорение за счёт квантования

**Проблемы:**
- Иногда не удавалось получить ответ от модели без видимых проблем в логах
- В перспективе планируется замена Ollama на другой инструмент (например VLLM, если появится сервер с GPU)

**Качество ответов:** Gemma-3-4b в большинстве случаев формулировала корректные и уместные рекомендации для инцидентов. Модель редко «галлюцинировала», генерируя посторонний контент: все выводы действительно основывались на предоставленном контексте.

### Применение LangChain и RAG

Для организации взаимодействия компонентов использовался фреймворк **LangChain**, который предоставляет абстракции для построения цепочек вызовов LLM с дополнением из внешних источников.

Сценарий работы RAG-агента был реализован как последовательность из нескольких шагов (Chain) с промежуточным хранением результатов. По сути, была сформирована связка «_Retriever – Analyzer – Action»_:
- **Retriever** выполняет векторный поиск похожих случаев
- **Analyzer** (LLM) на основе найденного анализирует ситуацию и формирует ответ
- **Action** доставляет этот ответ обратно в систему

**Пример кода:**
```python
def get_llm_response(query, context):
    # Настройка LangChain
    llm = OllamaLLM(model="gemma-3-4b-it.Q4_K_M:latest", base_url="http://localhost:11434", num_ctx=8192)
    
    # Создание промпта
    prompt_template = """
    Вы являетесь ИТ-специалистом, который обрабатывает ИТ-инциденты.
    
    На основе предоставленной информации о ранее закрытых инцидентах, дайте заключение о том, как лучше решить текущий инцидент.
    
    Ответ должен быть не более 3000 символов.
    
    Не повторяйтесь.
    
    Информация о ранее закрытых инцидентах:
    
    {context}
    
    Текущий инцидент:
    
    {query}
    
    Ваше заключение:
    """
    
    prompt = PromptTemplate(
        input_variables=["context", "incident"],
        template=prompt_template
    )
    
    # Создание цепочки
    chain = RunnableSequence(prompt | llm | StrOutputParser())
    
    # Обработка результата с помощью LLM
    llm_response = chain.invoke({"context": context, "query": query})
    
    return llm_response
```

## Статистика внедрения и эффективность

Разработанный RAG-агент был развёрнут как MVP **на части потока инцидентов** для тестирования. На момент подготовки статьи система обработала около **500 инцидентов**. Среднее время, затрачиваемое агентом на полный цикл (от получения данных до публикации комментария), составило **1–2 минуты на инцидент**.

**Ключевые метрики:**
- **Сокращение времени анализа:** ~10 минут экономии при анализе инцидента, более 80 часов в месяц экономии труда специалистов
- **Оптимизация штата:** высвобождение эквивалента 0,5 инженера за счет автоматизации
- **Снижение финансовых затрат:** экономия фонда оплаты труда оценочно достигает **200-300 тыс. рублей в месяц**
- **Ускорение решения инцидентов:** выгода от сокращения времени простоя может составлять **до нескольких миллионов рублей в месяц**
- **Затраты на поддержку:** не более чем в **100 тыс. руб/мес**
- **Окупаемость:** порядка **1-2 месяцев**

## Выводы и дальнейшее развитие

### Качество рекомендаций

В большинстве ситуаций советы агента корректны и полезны. Инженеры подтвердили, что полученные рекомендации часто совпадают с их собственными выводами, соответствуют реальным причинам проблем и помогают выработать способы решения.

### Проблемы и уроки

1. **Нерелевантные инциденты:** Иногда алгоритм подбирал нерелевантные инциденты в качестве похожих. Решение: настройка порога косинусной близости (снижение с 0.5 до 0.15). В дальнейшем планируется фильтрация по услугам и времени.

2. **Качество исходных данных:** Если текст решения в прошлом инциденте был заполнен формально (например: «Исправлено» или «Решение применено»), то такой кейс принесёт мало пользы. Нужно **улучшать качество закрытия инцидентов**. Например, регламентировать заполнение поля «Решение» или автоматически проверять тексты решений инцидентов с помощью той же LLM.

3. **Специфические услуги:** Для некоторых специфических услуг или систем даже семантический поиск не даёт результатов из-за малого числа примеров. Поэтому необходимо использовать и накапливать **базу знаний** по редким категориям: например, создавать отдельные векторные базы по каждому направлению, куда можно добавлять не только инциденты, но и связанные артефакты (документацию, FAQ) для повышения качества.

### Планы развития

- Улучшение стабильности и производительности (в том числе перенос вычислений на GPU-сервер при появлении такового)
- Качественное обновление LLM-модели и подключение полного потока инцидентов
- Автоматический анализ качества решений инцидентов
- Поиск скрытых закономерностей в причинах сбоев
- Эволюция в полноценного помощника SRE/DevOps-команды: не только подсказывать решения, но и проактивно уведомлять о повторяющихся проблемах, предлагать классификацию инцидентов, а возможно, и взаимодействовать с пользователями для сбора дополнительной информации

## Ключевые технические решения для реализации

### 1. Модель эмбеддинга
- **Модель:** ru-en-RoSBERTa
- **Размерность:** 1024
- **Библиотека:** SentenceTransformer

### 2. Векторная база данных
- **СУБД:** PostgreSQL с расширением pgvector
- **Тип вектора:** vector(1024)
- **Метрика поиска:** косинусное расстояние (<=>)
- **Порог релевантности:** 0.15 (после настройки)

### 3. LLM для переформулировки и генерации
- **Модель:** Gemma 3-4b-instruct
- **Квантование:** 4-бит
- **Инструмент:** Ollama
- **Использование:** 
  - Переформулировка описаний инцидентов
  - Генерация рекомендаций на основе найденных похожих случаев

### 4. Фреймворк
- **LangChain:** для организации RAG-цепочки

### 5. Поиск
- **Подход:** Двойной поиск (по оригинальному и переформулированному описанию)
- **Количество результатов:** топ-5 по каждому варианту (итого до 10, затем дедупликация)

### 6. Очистка HTML
- **Библиотека:** BeautifulSoup4

## Применимость к текущему проекту

Текущий проект использует:
- PostgreSQL с pgvector ✓
- Ollama для LLM ✓
- BGE-M3 для эмбеддингов (768 размерность)
- Qwen LLM для генерации

**Что можно применить:**
1. Переформулировка запросов с помощью LLM перед векторизацией
2. Двойной поиск (по оригинальному и переформулированному запросу)
3. Настройка порога релевантности (снижение с текущего 0.7 до более низкого значения, например 0.15-0.4)
4. Использование ru-en-RoSBERTa вместо BGE-M3 (требует изменения размерности вектора с 768 на 1024)
5. Использование LangChain для организации RAG-цепочки (опционально)

## Реализация в проекте

### Создание таблицы incidents

Создана новая таблица `incidents` на основе схемы из статьи с адаптацией под текущие данные CSV:

**Файл:** `init_incidents.sql`

**Основные поля:**
- `id` - первичный ключ
- `number` - номер инцидента (уникальный)
- `creation_date`, `resolution_date` - даты создания и решения
- `description` - описание инцидента (основное поле для поиска)
- `description_reformulated` - переформулированное описание через LLM
- `solution` - решение инцидента
- `embedding_description` - эмбеддинг оригинального описания (vector(1024))
- `embedding_reformulated` - эмбеддинг переформулированного описания (vector(1024))
- `llm_response` - ответ LLM с рекомендацией
- `metadata` - дополнительные метаданные из CSV (JSONB)

**Индексы:**
- Векторные индексы для обоих эмбеддингов (ivfflat)
- Индексы для быстрого поиска по номеру, дате, статусу, услуге
- GIN индекс для метаданных

**SQL функции для поиска:**
- `search_similar_incidents_by_description()` - поиск по оригинальному описанию
- `search_similar_incidents_by_reformulated()` - поиск по переформулированному описанию
- `search_similar_incidents_combined()` - объединенный поиск по обоим эмбеддингам

### Импорт данных из CSV

**Файл:** `scripts/import_incidents_from_csv.py`

**Функционал:**
- Импорт инцидентов из CSV файлов (INC.csv и другие)
- Маппинг полей CSV на поля таблицы incidents
- Очистка HTML из описаний (BeautifulSoup4)
- Парсинг дат в различных форматах
- Режимы импорта: append (добавить) и replace (заменить)
- Сохранение всех дополнительных полей в metadata (JSONB)

**Использование:**
```bash
python3 scripts/import_incidents_from_csv.py skuf/INC.csv --mode append
```

### Реализация функционала

**Основные компоненты:**
1. **Переформулировка запросов** - через LLM перед векторизацией
2. **Двойной поиск** - по оригинальному и переформулированному запросу
3. **Настройка порога** - использование порога 0.15 (как в статье)
4. **Интеграция с chat_app.py** - добавление нового эндпоинта для работы с инцидентами
